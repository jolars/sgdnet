<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model Families in sgdnet • sgdnet</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Model Families in sgdnet">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">sgdnet</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/algorithm.html">Technical Documentation</a>
    </li>
    <li>
      <a href="../articles/benchmarks.html">Benchmarks</a>
    </li>
    <li>
      <a href="../articles/cross-validation.html">Cross-Validation in sgdnet</a>
    </li>
    <li>
      <a href="../articles/introduction.html">An Introduction to sgdnet</a>
    </li>
    <li>
      <a href="../articles/models.html">Model Families in sgdnet</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/jolars/sgdnet">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Model Families in sgdnet</h1>
                        <h4 class="author">Johan Larsson</h4>
            
            <h4 class="date">2018-08-13</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/jolars/sgdnet/blob/master/vignettes/models.Rmd"><code>vignettes/models.Rmd</code></a></small>
      <div class="hidden name"><code>models.Rmd</code></div>

    </div>

    
    
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p><strong>sgdnet</strong> fits generalized linear models of the type</p>
<p><span class="math display">\[
\min_{\beta_0, \beta}
\left\{
-\frac1n \mathcal{L}\left(\beta_0,\beta; \mathbf{y}, \mathbf{X}\right)
+ \lambda \left[(1 - \alpha)||\beta||_2^2 + \alpha||\beta||_1 \right]
\right\},
\]</span> where <span class="math inline">\(\mathcal{L}(\beta_0,\beta; \mathbf{y}, \mathbf{X})\)</span> is the log-likelihood of the model, <span class="math inline">\(\lambda\)</span> is the regularization strength, and <span class="math inline">\(\alpha\)</span>, is the elastic net mixing parameter <span class="citation">(Zou and Hastie 2005)</span>, such that <span class="math inline">\(\alpha = 1\)</span> results in the lasso <span class="citation">(Tibshirani 1996)</span> and <span class="math inline">\(\alpha = 0\)</span> the ridge penalty.</p>
<p>When the lasso penalty is in place, the regularization imposed on the coefficients takes the shape of an octahedron when there are three coefficients.</p>
<div class="figure">
<img src="models_files/figure-html/unnamed-chunk-1-1.png" alt="The constraint region for the lasso penalty." width="700"><p class="caption">
The constraint region for the lasso penalty.
</p>
</div>
<p>Meanwhile, for ridge regression, this region has the shape of a ball.</p>
<div class="figure">
<img src="models_files/figure-html/unnamed-chunk-2-1.png" alt="Constraint region for the ridge penalty." width="700"><p class="caption">
Constraint region for the ridge penalty.
</p>
</div>
<p>The shape of the elastic net “ball” varies between these two extremes.</p>
<p>We can also use the so-called group lasso penalty on the coefficients. In the following figure, we show the constraint region for the group lasso penalty when <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_1\)</span> have been grouped.</p>
<div class="figure">
<img src="models_files/figure-html/unnamed-chunk-3-1.png" alt="Constraint region for the group lasso penalty." width="700"><p class="caption">
Constraint region for the group lasso penalty.
</p>
</div>
</div>
<div id="gaussian-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#gaussian-regression" class="anchor"></a>Gaussian regression</h2>
<p>For Gaussian (ordinary least squares) regression, we have the following objective</p>
<p><span class="math display">\[
\min_{\beta_0, \beta}
\left\{
  \frac{1}{n} \sum_{i=1}^n \left(y_i -\beta_0 - \beta^\intercal \mathbf{x}_i \right)^2
  + \lambda \left[(1 - \alpha)||\beta||_2^2 + \alpha||\beta||_1 \right]
\right\}.
\]</span></p>
<p>We’ll try to fit this model to the <a href="https://archive.ics.uci.edu/ml/datasets/abalone">Abalone</a> data set using the regular lasso (<code>alpha = 1</code>) – the default choice. The objective for this data set is to predict the weight of an abalone, a sea snail, using various physical attributes of some 4,177 specimen.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(sgdnet)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">gaussian_fit &lt;-<span class="st"> </span><span class="kw"><a href="../reference/sgdnet.html">sgdnet</a></span>(abalone<span class="op">$</span>x, abalone<span class="op">$</span>y, <span class="dt">family =</span> <span class="st">"gaussian"</span>)</a></code></pre></div>
<p>The explicit choice of family is strictly speaking irrelevant here since the Gaussian family is the default choice.</p>
<p>It is worth to mention that the predictors <em>sex</em> and <em>infant</em> are in fact dummy-coded variables from the same categorical predictor. It might make more sense to use a group lasso penalty here and group these predictors so that they are respectively included or excluded together.</p>
<p>Next, we plot the resulting model fits along the regularization path.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">plot</span>(gaussian_fit)</a></code></pre></div>
<div class="figure">
<img src="models_files/figure-html/unnamed-chunk-5-1.png" alt="A Gaussian lasso regression fit to the abalone data set" width="576"><p class="caption">
A Gaussian lasso regression fit to the abalone data set
</p>
</div>
<p>The deviance of this model is the residual sums of squares,</p>
<p><span class="math display">\[
RSS = \sum_{i=1}^{n} \left(y_{i} - \hat\beta_0 -  \hat\beta^\intercal \mathbf{x}_{i}\right)^2
\]</span></p>
<p>which we could retrieve for each fit using <code>deviance(fit_gaussian)</code>.</p>
</div>
<div id="binomial-logistic-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#binomial-logistic-regression" class="anchor"></a>Binomial logistic regression</h2>
<p>Binomial logistic regression is a natural solution to binary classification problems. Here, we model the log-likelihood ratio</p>
<p><span class="math display">\[
\log \Bigg[\frac{\text{P}(Y = 1 | X = x)}{\text{P}(Y = 0 | X = x)}\Bigg]  = \beta_0 + \beta^\intercal x,
\]</span> where <span class="math inline">\(Y \in \{0, 1\}\)</span>. To fit this model, <strong>sgdnet</strong> uses logistic binomial regression using the logit link, such that</p>
<p><span class="math display">\[
\log \left[ \frac{p(\mathbf{y})}{1-p(\mathbf{y})} \right] = \hat\beta_0 + \sum_{i=1}^n\hat\beta^\intercal \mathbf{x}_i.
\]</span></p>
<p>To fit this model using the elastic net penalty, we minimize the following convex objective:</p>
<p><span class="math display">\[
\min_{\beta_0, \beta}
\left\{
  -\frac1n \sum_{i=1}^n \bigg[y_i (\beta_0 + \beta^\intercal x_i) -
    \log\Big(1 + e^{\beta_0 + \beta^\intercal x_i}\Big)\bigg]
  + \lambda \left[(1 - \alpha)||\beta||_2^2 + \alpha||\beta||_1 \right]
\right\}.
\]</span></p>
<p>To illustate fitting the binomial logistic model with <strong>sgdnet</strong>, we’ll take a look at the <a href="http://archive.ics.uci.edu/ml/datasets/statlog+(heart)">Heart Disease</a> data set. In this set, we try to predict heart disease using a variety of clinical assessments such as blood pressure, heart rate, and electrocardiography results.</p>
<p>This time, we’ll employ ridge regression instead, setting <span class="math inline">\(\alpha = 0\)</span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">binomial_fit &lt;-<span class="st"> </span><span class="kw"><a href="../reference/sgdnet.html">sgdnet</a></span>(heart<span class="op">$</span>x, heart<span class="op">$</span>y, <span class="dt">family =</span> <span class="st">"binomial"</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw">plot</span>(binomial_fit)</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="kw">plot</span>(binomial_fit, <span class="dt">xvar =</span> <span class="st">"lambda"</span>)</a></code></pre></div>
<div class="figure">
<img src="models_files/figure-html/unnamed-chunk-6-1.png" alt="Binomial Regression on the Heart Disease Data Set." width="700"><img src="models_files/figure-html/unnamed-chunk-6-2.png" alt="Binomial Regression on the Heart Disease Data Set." width="700"><p class="caption">
Binomial Regression on the Heart Disease Data Set.
</p>
</div>
</div>
<div id="multinomial-logistic-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#multinomial-logistic-regression" class="anchor"></a>Multinomial logistic regression</h2>
<p>Multinomial logistic regression is concerned with classifying categorical outcomes using the multinomial likelihood. Here we use the loglinear representation</p>
<p><span class="math display">\[
\text{Pr}(Y_i = c) = 
  \frac{e^{\beta_{0_c}+\beta_c^\intercal \mathbf{x}_i}}{\sum_{k = 1}^K{e^{\beta_{0_k}+\beta_k^\intercal \mathbf{x}_i}}},
\]</span></p>
<p>which is overspecified in its non-regularized form, lacking a unique solution since we can shift the coefficients by a constant and still yield the same class probabilities. However, as in <strong>glmnet</strong> <span class="citation">(Friedman, Hastie, and Tibshirani 2010)</span> – which much of this packages functionality is modeled after – we rely on the regularization to take are of this <span class="citation">(Hastie, Tibshirani, and Wainwright 2015, 36–37)</span>. This works because, with the added regularization, the solutions are no longer indifferent to a shift in the coefficients.</p>
<p>The objective for the multinomial logistic regression is then</p>
<p><span class="math display">\[
\min_{\{\beta_{0_k}, \beta_k\}_1^K}
\left\{
  -\frac1n \sum_{i=1}^n \left[\sum_{k=1}^K y_{i_k} (\beta_{0_k}+\beta_k^\intercal \mathbf{x}_i) -
  \log \sum_{k=1}^K e^{\beta_{0_k}+\beta_k^\intercal \mathbf{x}_i}\right]
  + \lambda \left[(1 - \alpha)||\beta||_F ^2 + \alpha\sum_{j=1}^p||\beta_j||_q \right]
\right\}.
\]</span></p>
<p>where <span class="math inline">\(q = 1\)</span> invokes the standard lasso and <span class="math inline">\(q = 2\)</span> for the group lasso penalty.</p>
<p>The example for this model family comes from the <a href="https://archive.ics.uci.edu/ml/datasets/wine">Wine</a> data set where we attempt to classify a number of wines from Italy using the results of chemical analysis.</p>
<p>We will use the elastic net penalty this time, setting it (arbitrarily) at 0.8.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">multinomial_fit &lt;-<span class="st"> </span><span class="kw"><a href="../reference/sgdnet.html">sgdnet</a></span>(wine<span class="op">$</span>x, wine<span class="op">$</span>y, <span class="dt">family =</span> <span class="st">"multinomial"</span>)</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="kw">plot</span>(multinomial_fit, <span class="dt">layout =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure">
<img src="models_files/figure-html/unnamed-chunk-7-1.png" alt="Multinomial logistic regression on the wine data set." width="672"><p class="caption">
Multinomial logistic regression on the wine data set.
</p>
</div>
</div>
<div id="multivariate-gaussian-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#multivariate-gaussian-regression" class="anchor"></a>Multivariate gaussian regression</h2>
<p>Multivariate gaussian models, fit using <code>family = "mgaussian"</code> in the call to <code><a href="../reference/sgdnet.html">sgdnet()</a></code>, are useful when there are several correlated and numeric responses. The objective for this model is to minimize</p>
<p><span class="math display">\[
\frac{1}{2n} ||\mathbf{Y} -\mathbf{B}_0\mathbf{1} - \mathbf{B} \mathbf{X}||^2_F
 + \lambda \left((1 - \alpha)/2||\mathbf{B}||_F^2 + \alpha ||\mathbf{B}||_{12}\right),
\]</span> where <span class="math inline">\(\mathbf{Y}\)</span> is a matrix of responses, <span class="math inline">\(\mathbf{B}\)</span> is a matrix of coefficients, and <span class="math inline">\(\mathbf{1}\)</span> is a matrix of all ones.</p>
<p><span class="math inline">\(\alpha ||\mathbf{B}||_{12}\)</span> in the penalty-part of the objective is the <em>group lasso</em>, which enables feature selection such that each column of coefficients in <span class="math inline">\(\mathbf{B}\)</span> each either in or out simultaneously. The proximal operator for the group lasso is given by</p>
<p><span class="math display">\[
\beta_j \leftarrow \left( 1 - \frac{\gamma\lambda\alpha}{||\beta_j||_2} \right)_+\beta_j,
\]</span> where the subscript <span class="math inline">\(+\)</span> indicates the positive part of <span class="math inline">\((\cdot)\)</span> and <span class="math inline">\(\gamma\)</span> is the step size. <span class="math inline">\(\beta\)</span> is the vector of coefficients corresponding to response <span class="math inline">\(j\)</span>.</p>
<p>Our example data set for this model comes in the shape of student performance data. We have two responses indicating the final grade of a student in maths and portugese respectively.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">mgaussian_fit &lt;-<span class="st"> </span><span class="kw"><a href="../reference/sgdnet.html">sgdnet</a></span>(student<span class="op">$</span>x, student<span class="op">$</span>y, <span class="dt">family =</span> <span class="st">"mgaussian"</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">plot</span>(mgaussian_fit)</a></code></pre></div>
<div class="figure">
<img src="models_files/figure-html/unnamed-chunk-8-1.png" alt="Multivariate Gaussian regression on the Student Performance Data Set." width="672"><p class="caption">
Multivariate Gaussian regression on the Student Performance Data Set.
</p>
</div>
<p>This family has an additional parameter, <code>standardize.response</code>, which enables us to standardize the response as well but, given that the responses are already on the same scale, we refrain.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-friedman2010">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” <em>Journal of Statistical Software</em> 33 (1): 1–22. <a href="https://doi.org/10.18637/jss.v033.i01" class="uri">https://doi.org/10.18637/jss.v033.i01</a>.</p>
</div>
<div id="ref-hastie2015">
<p>Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. 1 edition. Boca Raton: Chapman and Hall/CRC.</p>
</div>
<div id="ref-tibshirani1996">
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 58 (1): 267–88.</p>
</div>
<div id="ref-zou2005">
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society. Series B (Statistical Methodology)</em> 67 (2): 301–20.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#overview">Overview</a></li>
      <li><a href="#gaussian-regression">Gaussian regression</a></li>
      <li><a href="#binomial-logistic-regression">Binomial logistic regression</a></li>
      <li><a href="#multinomial-logistic-regression">Multinomial logistic regression</a></li>
      <li><a href="#multivariate-gaussian-regression">Multivariate gaussian regression</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Johan Larsson, Toby Dylan Hocking, Michael Weylandt.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
