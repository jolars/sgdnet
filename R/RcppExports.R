# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Deviance
#'
#' Computes the deviance of the model given by `weights` and `intercept`.
#'
#' @param x a feature matrix (dense or sparse)
#' @param y a response vector
#' @param weights a vector of coefficients
#' @param intercept an intercept vector
#' @param is_sparse whether x is sparse
#' @param n_samples the number of samples
#' @param n_feature the number of features
#' @param n_classes the number of classes
#'
#' @return Returns the deviance.
#'
#' @noRd
#' @keywords internal
NULL

#' Rescale weights and intercept before returning these to user
#'
#' Currently no processing, and therefore no rescaling, is done
#' when the intercept is fit. If x is sparse.
#'
#' @param weights weights
#' @param weights_archive storage for weights
#' @param intercept intercept
#' @param intercept_archive storage for intercepts on a per-penalty basis
#' @param x_center the offset (mean) used to possibly have centered x
#' @param x_scale the scaling that was applied to x
#' @param y_center the offset (mean) that y was offset with
#' @param y_scale scaling for y
#' @param n_features the number of features
#' @param n_classes number of classes
#' @param fit_intercept whether to fit the intercept
#'
#' @return `weights` and `intercept` are rescaled and stored in weights_archive
#'   and intercept_archive.
#'
#' @noRd
NULL

#' Compute Regularization Path
#'
#' This function computes the regularization path as in glmnet so that
#' the first solution is the null solution (if elasticnet_mix != 0).
#'
#' @param lambda lambda values in input -- empty by default
#' @param n_lambda required number of penalties
#' @param lambda_min_ratio smallest lambda_min_ratio
#' @param elasticnet_mix ratio of l1 penalty to l2. Same as alpha in glmnet.
#' @param x feature matrix
#' @param y response vector
#' @param n_samples number of samples
#' @param alpha l2-penalty
#' @param beta l1-penalty
#' @param y_scale scale of y, used only to return lambda values to same
#'   scale as in glmnet
#'
#' @return lambda, alpha and beta are updated.
#'
#' @noRd
NULL

#' Adapative transposing of feature matrix
#'
#' For sparse matrices, armadillo does not (yet?) have a inplace
#' transpose method, so we overload for sparse and dense matrices,
#' transposing inplace when x is dense.
#'
#' @param x a sparse or dense matrix
#'
#' @return x transposed.
#'
#' @keywords internal
#' @noRd
NULL

#' Setup sgdnet Model Options
#'
#' Collect parameters from `control` and setup storage for coefficients,
#' intercepts, gradients, and more so that we can iterate along the
#' regularization path using warm starts for successive iterations.
#'
#' @param x features
#' @param y response
#' @param is_sparse whether x is sparse or not
#' @param control a list of control parameters
#'
#' @return See [SgdnetCpp].
#'
#' @noRd
#' @keywords internal
NULL

#' Fit a Model with sgdnet
#'
#' This main use of this function is calling the templated SetupSgdnet()
#' so that the dense and sparse implementations are compiled and
#' called appropriately. The control parameters in `control` are just
#' passed along.
#'
#' @param x_in feature matrix
#' @param y response matrix
#' @param control a list of control parameters
#'
#' @return A list of
#'   * ao: the intercept,
#'   * beta: the weights,
#'   * losses: the loss at each outer iteration per fit,
#'   * npasses: the number of effective passes (epochs) accumulated over,
#'     all lambda values, and
#'   * return_codes: the convergence result. 0 mean that the algorithm
#'     converged, 1 means that `max_iter` was reached before the algorithm
#'     converged.
#'
#' @keywords internal
SgdnetCpp <- function(x_in, y, control) {
    .Call(`_sgdnet_SgdnetCpp`, x_in, y, control)
}

