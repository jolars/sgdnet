% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sgdnet.R
\name{sgdnet}
\alias{sgdnet}
\alias{sgdnet.default}
\title{Fit a Generalized Linear Model with Elastic Net Regularization}
\usage{
sgdnet(x, ...)

\method{sgdnet}{default}(x, y, family = c("gaussian", "binomial",
  "multinomial", "mgaussian"), alpha = 1, gamma = 3, nlambda = 100,
  lambda.min.ratio = if (NROW(x) < NCOL(x)) 0.01 else 1e-04,
  lambda = NULL, penalty = "", maxit = 1000, standardize = TRUE,
  intercept = TRUE, thresh = 0.001, standardize.response = FALSE,
  ...)
}
\arguments{
\item{x}{input matrix}

\item{...}{ignored}

\item{y}{response variable}

\item{family}{reponse type, one of \code{'gaussian'}, \code{'binomial'},
\code{'multinomial'}, or \code{'mgaussian'}. See \strong{Supported families} for details.}

\item{alpha}{elastic net mixing parameter}

\item{gamma}{the non-convexity parameter for MCP and SCAD}

\item{nlambda}{number of penalties in the regualrization path}

\item{lambda.min.ratio}{the ratio between \code{lambda_max} (the smallest
penalty at which the solution is completely sparse) and the smallest
lambda value on the path. See \strong{Regularization Path} for details.}

\item{lambda}{regularization strength}

\item{penalty}{the regularization penalty, should be specified if MCP and SCAD is used}

\item{maxit}{maximum number of effective passes (epochs)}

\item{standardize}{whether to standardize \code{x} or not}

\item{intercept}{whether to fit an intercept or not}

\item{thresh}{tolerance level for termination of the algorithm. The
algorithm terminates when
\deqn{
    \frac{|\beta^{(t)}
    - \beta^{(t-1)}|{\infty}}{|\beta^{(t)}|{\infty}} < \mathrm{thresh}
  }{
    max(change in weights)/max(weights) < thresh.
  }}

\item{standardize.response}{whether \code{y} should be standardized for
\code{family = "mgaussian"}}
}
\value{
An object of class \code{'sgdnet'} with the following items:
\item{\code{a0}}{the intercept}
\item{\code{beta}}{the coefficients stored in sparse matrix format
"dgCMatrix". For the multivariate families, this is a
list with one matrix of coefficients for each response or
class.}
\item{\code{nulldev}}{the deviance of the null (intercept-only model)}
\item{\code{dev.ratio}}{the fraction of deviance explained, where the deviance
is two times the difference in loglikelihood between the
saturated model and the null model}
\item{\code{df}}{the number of nozero coefficients along the
regularization path. For \code{family = "multinomial"},
this is the number of variables with
a nonzero coefficient for any class.}
\item{\code{dfmat}}{a matrix of the number of nonzero coefficients for
any class (only available for multivariate models)}
\item{\code{alpha}}{elastic net mixing parameter. See the description
of the arguments.}
\item{\code{lambda}}{the sequence of lambda values scaled to the
original scale of the input data.}
\item{\code{nobs}}{number of observations}
\item{\code{npasses}}{accumulated number of outer iterations (epochs)
for the entire regularization path}
\item{\code{offset}}{a logical indicating whether an offset was used}
\item{\code{grouped}}{a logical indicating if a group lasso penalty was used}
\item{\code{call}}{the call that generated this fit}
}
\description{
Fit a Generalized Linear Model with Elastic Net Regularization
}
\section{Model families}{

Three model families are currently supported: gaussian univariate
regression, binomial logistic regression, and multinomial logistic
regression. The choice of which is made
using the \code{family} argument. Next follows the objectives of the various
model families:

\emph{Gaussian univariate regression}:

\deqn{
  \frac{1}{2n} \sum_{i=1}^n (y_i - \beta_0 - x_i^\mathsf{T} \beta)^2
  + \lambda \left( \frac{1 - \alpha}{2} ||\beta||_2^2
                   + \alpha||\beta||_1 \right).
}{
  1/(2n) \sum (y - \beta_0 - x^T \beta)^2
  + \lambda [(1 - \alpha)/2 ||\beta||_2^2 + \alpha||\beta||_1].
}

\emph{Binomial logistic regression}:

\deqn{
  -\frac1n \sum_{i=1}^n \bigg[y_i (\beta_0 + \beta^\mathsf{T} x_i) -
    \log\Big(1 + e^{\beta_0 + \beta^\mathsf{T} x_i}\Big)\bigg]
  + \lambda \left( \frac{1 - \alpha}{2} ||\beta||_2^2
                   + \alpha||\beta||_1 \right),
}{
  -1/n \sum_{i=1}^n {y_i (\beta_0 + \beta^T x_i) -
    log[1 + exp(\beta_0 + \beta^T x_i)]}
  + \lambda [(1 - \alpha)/2 ||\beta||_2^2 + \alpha||\beta||_1],
}
where \eqn{y_i \in \{0, 1\}}{y ~ {0, 1}}.

\emph{Multinomial logistic regression}:
\deqn{
 -\bigg\{\frac1n \sum_{i=1}^n \Big[\sum_{k=1}^m y_{i_k} (\beta_{0_k} + x_i^\mathsf{T} \beta_k) -
 \log \sum_{k=1}^m e^{\beta_{0_k}+x_i^\mathsf{T} \beta_k}\Big]\bigg\}
 + \lambda \left( \frac{1 - \alpha}{2}||\beta||_F^2 + \alpha \sum_{j=1}^p ||\beta_j||_q \right),
}{
 -{1\n \sum_{i=1}^n [\sum_{k=1}^m y_{i_k} (\beta_{0_k} + x_i^T \beta_k) -
 \log \sum_{k=1}^m exp(\beta_{0_k}+x_i^T \beta_k)]}
 + \lambda ((1 - \alpha)/2||\beta||_F^2 + \alpha \sum_{j=1}^p ||\beta_j||_q),
}
where \eqn{q \in {1, 2}}{q = {1,2}} invokes the standard lasso and 2 the
group lasso penalty respectively, \eqn{F} indicates the Frobenius norm,
and \eqn{p} is the number of classes.

\emph{Multivariate gaussian regression}:
\deqn{
  \frac{1}{2n} ||\mathbf{Y} -\mathbf{B}_0\mathbf{1} - \mathbf{B} \mathbf{X}||^2_F
  + \lambda \left((1 - \alpha)/2||\mathbf{B}||_F^2 + \alpha ||\mathbf{B}||_{12}\right),
}{
  1/(2n) ||Y - B_01 - BX||_F^2 + \lambda (1 - \alpha)/2||B||_F^2 + \alpha ||B||_12),
}
where \eqn{\mathbf{1}}{1} is a vector of all zeros, \eqn{\mathbf{B}}{B} is a
matrix of coefficients, and \eqn{||\dot||_{12}}{||.||_12} is the mixed
\eqn{\ell_{1/2}}{L1/2} norm. Note, also, that Y is a matrix
of responses in this form.
}

\section{Regularization Path}{

The default regularization path is a sequence of \code{nlambda}
log-spaced elements
from \eqn{\lambda_{\mathrm{max}}}{lambda_max} to
\eqn{\lambda_{\mathrm{max}} \times \mathtt{lambda.min.ratio}}{
     lambda_max*lambda.min.ratio},
For the gaussian family, for instance,
\eqn{\lambda_{\mathrm{max}}}{lambda_max} is
the largest absolute inner product of the feature vectors and the response
vector,
\deqn{\max_i \frac{1}{n}|\langle\mathbf{x}_i, y\rangle|.}{
      max |<x, y>|/n.}
}

\section{Relationship with glmnet}{

\strong{sgdnet} is modeled to resemble \link[glmnet:glmnet-package]{glmnet} closely so that users
can expect to receive more or less equivalent output regardless of whether
\code{\link[=sgdnet]{sgdnet()}} or \code{\link[glmnet:glmnet]{glmnet::glmnet()}} is called. Nevertheless, there are a
few instances where we have decided to diverge from the behavior of
\link[glmnet:glmnet-package]{glmnet}:
\itemize{
\item When the ridge penalty is used (\code{alpha = 0}), and a regularization
path (\eqn{\lambda~s}{lambdas}) is automatically generated,
\code{\link[glmnet:glmnet]{glmnet::glmnet()}} fits the null model as the start of the path
(as if \eqn{\lambda = \infty}{lambda = inf})
even though the first \eqn{\lambda}{lambda} reported actually
doesn't yield this fit. In \link[=sgdnet-package]{sgdnet}, we have opted
to fit the model so that it is true to the path that is returned.
}
}

\examples{
# Gaussian regression with sparse features with ridge penalty
fit <- sgdnet(abalone$x, abalone$y, alpha = 0)

# Binomial logistic regression with elastic net penalty, no intercept
binom_fit <- sgdnet(heart$x,
                    heart$y,
                    family = "binomial",
                    alpha = 0.5,
                    intercept = FALSE)

# Multinomial logistic regression with lasso
multinom_fit <- sgdnet(wine$x, wine$y, family = "multinomial")

# Multivariate gaussian regression
mgaussian_fit <- sgdnet(student$x, student$y, family = "mgaussian")
}
\seealso{
\code{\link[=predict.sgdnet]{predict.sgdnet()}}, \code{\link[=plot.sgdnet]{plot.sgdnet()}}, \code{\link[=coef.sgdnet]{coef.sgdnet()}},
\code{\link[=sgdnet-package]{sgdnet-package()}}
}
